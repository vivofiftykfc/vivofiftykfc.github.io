<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>无人机安装指南</title>
    <link href="/2025/04/07/%E6%9C%AA%E5%91%BD%E5%90%8D/"/>
    <url>/2025/04/07/%E6%9C%AA%E5%91%BD%E5%90%8D/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>数学建模</category>
      
      <category>评价类模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>无人机开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客搭建及工作流优化</title>
    <link href="/2025/04/07/%E6%8A%98%E8%85%BE%E6%9D%82%E9%A1%B9/2025-04-07-%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
    <url>/2025/04/07/%E6%8A%98%E8%85%BE%E6%9D%82%E9%A1%B9/2025-04-07-%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<ul><li>[数学,数学建模,评价类模型]</li></ul>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>折腾的小杂项</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>语音转文字导论-TTS</title>
    <link href="/2025/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%B9%E5%90%91%E5%B1%95%E6%9C%9B/2025-04-06-%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E5%AD%97%E5%AF%BC%E8%AE%BA-TTS/"/>
    <url>/2025/04/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%96%B9%E5%90%91%E5%B1%95%E6%9C%9B/2025-04-06-%E8%AF%AD%E9%9F%B3%E8%BD%AC%E6%96%87%E5%AD%97%E5%AF%BC%E8%AE%BA-TTS/</url>
    
    <content type="html"><![CDATA[<h1 id="熵权法-ewm">熵权法-EWM</h1><h1 id="语音转文字-tts">语音转文字-TTS</h1>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
      <category>语音转文字</category>
      
    </categories>
    
    
    <tags>
      
      <tag>语音转文字</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>显示器参数及相关知识解读</title>
    <link href="/2025/04/06/%E7%94%B5%E8%84%91%E7%A1%AC%E4%BB%B6/%E6%A6%82%E5%BF%B5%E9%98%90%E8%BF%B0/2025-04-05-%E6%98%BE%E7%A4%BA%E5%99%A8%E5%8F%82%E6%95%B0%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E8%A7%A3%E8%AF%BB/"/>
    <url>/2025/04/06/%E7%94%B5%E8%84%91%E7%A1%AC%E4%BB%B6/%E6%A6%82%E5%BF%B5%E9%98%90%E8%BF%B0/2025-04-05-%E6%98%BE%E7%A4%BA%E5%99%A8%E5%8F%82%E6%95%B0%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="显示器相关参数与相关原理解读">显示器相关参数与相关原理解读</h1><p>最近买了个副屏，顺便写个文章整一下相关知识方便选购。</p><h2 id="彩色显示器原理">彩色显示器原理</h2><h3 id="前置知识">前置知识</h3><ul><li>液晶材料：一种合成的有机化合物，处于固态与液态之间，不通电时内部分子排列混乱不透光，通电时分子排列有秩序透光，当然也有相反的情况，通电不透光不通电透光。</li><li>光的偏振：光的偏振是横波振动方向的空间选择，通过偏振片的调控与检测光强变化，我们可以验证其横波特性及振动方向选择性。具体详见高中物理书。</li></ul><h3 id="主要思想">主要思想</h3><p>控制RGB值来混合出彩色出来。</p><h3 id="技术路线">技术路线</h3><h4 id="lcd">LCD</h4><h5 id="普通lcd">普通LCD</h5><p>这种还是比较主流也比较老的，最下面是一层背光层，发白光，底光层上面蒙上一层具体就是在那种相互垂直的两个偏振片中间加上一个液晶层，利用液晶对光的偏转能力来控制其光的偏转量，进而控制亮度。而彩色的话就在液晶层外加上彩色滤光片就行。普通的背光板就是反光板匀光板中间夹一层led灯带。当然在显示器打开的情况下显示黑屏的话，因为底光已经开了，同时液晶层在不通电的时候实际上也会对光有一部分偏转，故与不通电的纯黑色有区别，同时也会造成对比度的损失。</p><p>具体可以根据液晶层的默认状态分为下面三种：</p><ul><li>VA默认垂直，加电呈垂直螺旋状偏转透光，对比度高，可视角度一般，响应较慢，有些劣质的板子可能会有拖影，适合电影和3A用户。</li><li>IPS默认水平，加压水平平行偏转透光，对比度一般，可视角度广，响应适中，算是水桶板子，适合大多数场景。</li><li>TN默认水平螺旋偏转，加压垂直偏转不透光，对比度低，可视角度差，但是响应快，适合go学长瓦学弟。</li></ul><h5 id="qled">QLED</h5><p>这个是把LCD的基础上改进背光为蓝色，然后使用量子点涂料（量子点是一种极为微小的化合物，收蓝光之后会发射波长不同的光线，有点电子激发再回落基态的味道了）刷在基板上，解决的是普通LCD饱和度不行的问题，实际上还是归属于LCD。</p><h5 id="mini-led">mini-LED</h5><p>将背光板换成分块，在黑色区域直接关背光，其他和LCD一样，有利于高阶HDR的实现，但是有会出现调教不好有拖影或者光晕的问题。</p><h4 id="led">LED</h4><h5 id="oled">OLED</h5><p>有机发光二极管（Organic Light EmittingDiode），传统二极管好是好，但是每个像素上三个二极管，再集成为一块大屏幕就难以做到，于是就有了这个东西。用的是直接给荧光材料通电来实现发有颜色的光，三个集成就是一个像素。也可以看作像素级别的分区背光，无光晕且饱和度高，厚度薄，响应快，色域广，没背光功耗小，有柔性。缺点就是因为三种荧光材料寿命不同，容易烧屏（色彩残留，有色差），亮度峰值低，PWM调光、等效分辨率下降的问题。</p><h5 id="w-oled">W-OLED</h5><p>将三种荧光材料换成白色荧光材料，上面盖上RGBW偏振片（红绿蓝加上一个白），其中W是为了弥补亮度，但是W又对红绿蓝像素产生了白色光干扰，又回到了LCD的问题上了。</p><h5 id="qd-oled">QD-OLED</h5><p>将QLED里面的背光换成了蓝色荧光物质，还是属于LED里面，解决的是W-OLED的问题，但是量子点材料可能被外界光激发污染信号。</p><h5 id="micro-oled">micro-OLED</h5><p>微型的发光二极管，就是解决了传统每个像素上三个二极管，再集成为一块大屏幕就难以做到的问题，但是因为巨量转移技术不成熟所导致的价格贵，一块几十万。</p><h2 id="参数">参数</h2><h3 id="屏幕尺寸">屏幕尺寸</h3><p>指屏幕对角线的长度，单位通常是英寸（下面都直接写寸）。主流的那种十分正常的屏幕有24、27、32寸等，便携屏目前我见的（也不多）都14寸不等，一般来说24或者27比较主流，一般网吧差不多就这样，大一点的话可以选更大屏幕或者带鱼屏（富哥随意）。</p><h3 id="分辨率">分辨率</h3><p>平常见的最多的应该就是1080p（1920*1080）、2k（2560*1440，即16:9）、4k（3840*2160），有的时候还能看到像2.5k（16:10的主配，当然也有的人叫2560*1440为2.5k）。当然这只是常见的，很多情况下什么2k2.5k的叫法十分随意，其他的见下表（图源小黑盒）</p><figure><imgsrc="D:\blogs\source\img\inblogs\76a78db4df63fe2a9e93637edd70ab7.jpg"alt="76a78db4df63fe2a9e93637edd70ab7" /><figcaptionaria-hidden="true">76a78db4df63fe2a9e93637edd70ab7</figcaption></figure><p>还有就是这个玩意儿相当于乱叫，不能太深究，游戏群体，显示器厂家，电影行业，电视行业叫法全都不一个标准。下面举几个例子：</p><ul><li><p>常见的视频平台：标清是480p，高清是720p，超高清是1080p，蓝光是1080p60帧，4k是2160p，码率未知。</p></li><li><p>上海某电视台剪辑师说法：1080P10M叫高清，2560*1440M叫2K，3840*2160或4096*2160叫4K，而蓝光是特指蓝光盘，和磁带一样都是实体存储介质。</p></li><li><p>山东某电视台工作人员说法：480p为标准，720p为高清，1080p是超清。4k的就叫做4k。码率按mxf算。</p></li></ul><p>对于视频而言，不能光看分辨率，码率也很关键。视频分辨率是4k的，但码率如果是5m，那还不如10m的1080p清晰。再一个对于显示器而言，也不能光看分辨率，还要看ppi。国内视频不如自己下的资源清晰是因为码率太低了。具体的诸如找BT种子的相关资源网站或许之后有时间的话也会整理一下？</p><p>有些重度影视发烧友，甚至还要用PotPlayer软件调试视频色彩平衡还有什么解码方式，这个算是逛论坛的时候看到的。</p><p>显示上还有一点就是如果你用一张1080P照片，放到2K分辨率下，可能会出现更模糊的情况，1080P没有那么多精准像素点，靠2K屏模拟，所以有点胡，当然了，感知度不强。</p><p>有的网友指出OLED屏幕的分辨率要打八折，是像素的工作原理不同而导致的。</p><h4 id="基本单位">基本单位</h4><ul><li><p>px（像素）：像素是显示器上最小的显示单位，用来表示图像的基本构成。px就是指一个像素点。</p></li><li><p>pt（点）：描述字体大小。在数字设备上，pt 与 px的转换通常依赖于设备的分辨率或 ppi（如 72ppi 或 96ppi）。</p></li><li><p>ppi（每英寸像素数）：用来衡量显示屏的像素密度，即每英寸包含的像素数。公式为：</p></li></ul><p><span class="math display">\[PPI = \frac{\sqrt{(宽度像素数)^2 +(高度像素数)^2}}{屏幕对角线长度（英寸）}\]</span></p><ul><li>dpi（每英寸点数）描述打印机输出的精细程度，表示每英寸能够打印的点数。有时dpi 和 ppi 会混淆，但严格来说，dpi 更多用于打印领域，而 ppi则用于显示设备。</li></ul><h3 id="面板选择">面板选择</h3><p>具体详见发光原理，一般来说直接IPS或者OLED，玩3A和电影的可以考虑VA，高端FPS上TN.</p><h3 id="刷新率">刷新率</h3><p>就是帧数，越高越流畅，没啥好讲的，像我这种就敲敲键盘打打gal的上个60Hz够了，打电动的尤其是fps玩家越高越好。</p><h3 id="亮度">亮度</h3><h4 id="单位">单位</h4><p>首先明确一点就是在显示器的亮度上一般用的是尼特（nit）来刻画单位光强密度，在其他地方可能还会见到诸如：</p><ul><li>坎德拉（光强的国际制七个主单位之一，光源在某一方向上的发光强度）：衡量光源在某一方向上的发光强度，大概可以拿以点光源的光强除上以点光源为圆心的包络球的球面度来理解。1坎德拉是光源在给定方向（频率540×10¹²Hz，对应绿光）发出1/683瓦特每球面度的辐射强度时产生的发光强度。</li><li>烛光：旧单位，比坎德拉稍小，约的话可以直接等于坎德拉，现在基本上被废。</li><li>流明（光照强度之和，整体发出的可见光总量）：1流明 =1坎德拉×1球面度（lm = cd·sr）</li><li>尼特（光源或反射表面单位面积的发光强度）：1尼特 = 1坎德拉/平方米（nt= cd/m²）</li><li>勒克斯（照度的单位，表示的单位面积接收到的光通量）：1勒克斯 =1流明/平方米（lx = lm/m²）</li></ul><p>总的来说光通量（流明）是指光源所有方向上发射出光的能力，发光强度（坎德拉）是特定方向上的发光能力，亮度（尼特）是单位面积的发出或反射的发光强度，照度（勒克斯）是单位面积所入射的光量。</p><h4 id="例子">例子</h4><p>电影院中屏幕大概是50nit，户外手机一般为300nit往上，所以电影院不能看手机。当然，要是算流明的话电影院屏幕远远大于手机</p><p>索尼HDR电视的亮度能干到10000nit，大多数HDR设备最高2000nit，再高就是瞎眼神器。</p><p>智能手机/平板电脑：200 至 1000+尼特，400-500可以在白天看，200nit太阳下看不清。</p><p>笔记本电脑/显示器：200 至 600+尼特，不建议太高，上限200低端，上限400足够，看不清建议拉窗帘而不是调亮度。笔者刚买的副屏就是400nit的。</p><h3 id="对比度">对比度</h3><p>对比度指的是一幅图像中明暗区域最亮的白和最暗的黑之间的比值，差异范围越大代表对比越大，差异范围越小代表对比越小。对比度高，可以展示更多的亮部和暗部细节，让画面更有层次。</p><p>对比度的相关判断可以直接点击<ahref="https://www.pingmudiy.com/screentest/#welcome">这个链接</a>，用烧屏网的相关工具进行测试。这里就不放图了，避免莫名其妙的压缩导致不准。</p><h3 id="接口">接口</h3><ul><li><p><strong>VGA</strong>：上古模拟信号接口，上次见还是家里面的古董老机器，仅限视频传输，容易受到干扰，信号容易失真，字容易糊仅支持低分辨率（如1080p@60Hz），逐渐被淘汰，但是在几十年前十分风光。</p></li><li><p><strong>DVI</strong>：为了弥补 VGA接口的不足，推出了能同时支持模拟信号和数字信号传输的 DVI接口，能干到1080p@144Hz，但是缺点就是不支持音频信号。现在也基本上见不到了，就不再详细说明。</p></li><li><p><strong>HDMI</strong>：主流数字接口，支持音画同步传输。HDMI2.0带宽18Gbps，可承载1440p@144Hz、1080p@240Hz以及4K<spanclass="citation" data-cites="60Hz">@60Hz</span>；HDMI2.1带宽48Gbps，支持4K@120Hz或8K<span class="citation"data-cites="60Hz">@60Hz</span>，增加了对动态HDR的支持，适用于游戏主机与高端显卡。</p></li><li><p><strong>DisplayPort（DP）</strong>：PC领域专业接口，DP1.4支持4K@120HzHDR或8K@60Hz HDR，支持3:1的DSC压缩技术；DP2.0带宽提升至80Gbps，可原生支持8K@60Hz。</p></li><li><p><strong>USB-C</strong>：多功能接口，支持视频传输（需DP AltMode）、数据传输与充电（PD协议）。部分显示器通过USB-C直连笔记本实现「一线通」，但需注意带宽限制（如USB3.2 Gen2仅支持4K@60Hz）。</p></li></ul><p>电竞用户优先选DP 1.4或HDMI2.1及以上；轻薄本用户也可考虑USB-C一线通；多设备切换需求可关注显示器是否配备KVM功能。</p><p>具体而言可以用接口带宽来算一下所支持的分辨率，公式如下： <spanclass="math display">\[总像素*子像素*色深*刷新率 = 总带宽\]</span> 其中： <span class="math display">\[总像素=横向分辨率*纵向分辨率\]</span></p><p><span class="math display">\[子像素=RGB色素数=3\]</span></p><p>假设要输出4K、10bit、144Hz的画面： <span class="math display">\[3840*2160*3*10bit*144 = 35.8Gbps。\]</span></p><h3 id="色域">色域</h3><h4 id="概述">概述</h4><p>就是能够显示的色彩范围，越高表现度越好，标准的话主流的有NTSC（有点老）、sRGB、P3和AdobeRGB，一般来说看sRGB就行，日常95%以上，设计相关或者有其他偏好的话就上99%或者100%sRGB的就好。想要进一步了解更多就继续看</p><h4 id="拓展">拓展</h4><ul><li><strong>sRGB</strong>：由微软公司和惠普公司主导，是windows系统的UI和各类软件中默认应用的色域，也是互联网中图形图像信息的标准色域。但是sRGB色域在CIE-1931中覆盖面积不高，色彩还原度有限，绿色部分的覆盖率较低，目前市面上大部分显示器都能轻松达到96%以上的sRGB色域覆盖。</li><li><strong>Adobe RGB</strong>：Adobe公司在sRGB不够用的时候出的，玩摄影剪辑的都比较熟悉，他家的PS、AI、PR之类的十分流行。其色域覆盖区域更广，显示色彩更丰富，大多应用于高端的艺术设计专业显示器中，价格昂贵。</li><li><strong>DCI-P3色域</strong>：其主要由美国电影行业提出，是数字影像领域中比较新的色域标准，着重人类视觉的真实体验，匹配电影的色彩表现要求。相比sRGB色域，其覆盖了更多的红色和绿色空间。但是我见的倒是相对少一些。</li><li><strong>Rec.2020</strong>：国际电信组织将其作为一个涵盖各影像广播的参数规划、视其为高画质投影机/电视和相关无线电视广播设备的参考标准。这些包含：解析度(resolution)、帧速率(framerate)、位元深度(bit depth)、色域等。</li><li><strong>P3</strong>：广义上包含了 DCI-P3 和 DisplayP3，前者是影视行业标准，后者是 Apple 在 DCI-P3 基础上参考了 sRGB而修订出的自己的标准，在白点和伽马上与 DCI-P3不同，但是能覆盖的颜色相同，相对于 sRGB 除了蓝色都有较大提升。</li><li><strong>NTSC</strong>：由美国国家电视标准委员会在 1953年订制，目的是为了给当时刚出现不久的 CRT彩色电视定制一套标准，由于实在是太过于古老（Apple DOS 3.1 诞生于 1978年， MS-DOS 诞生于 1980 年）早已不适用于现代显示器，更最重要的是对于PC（广义的）和移动设备来说，几乎没有内容创作者是以 NTSC为工作空间的，它保留下来最多的用途还是用于比较其他的色彩空间。</li><li><strong>REC-709</strong>：是 sRGB的影视行业（高清电视）名称，属于早期彩色电视所用的色域标准，也是目前最为广泛的色域标准，我国影视行业至今仍在使用此标准。</li></ul><p>其中AdobeRGB和DCI-P3色域远大于sRGB，显示的色彩范围更丰富，也被称为广色域。一般来说达到92%DCI-P3就可以说是广色域屏幕了。</p><figure><imgsrc="https://pic2.zhimg.com/v2-3a55b4540248ffc6c308c23af36f4181_r.jpg"alt="【科普】电视显示器色域科普：sRGB，DCI-P3，Adobe RGB色域，Rec.2020,广色域都是些啥？一文读懂 - 知乎" /><figcaptionaria-hidden="true">【科普】电视显示器色域科普：sRGB，DCI-P3，AdobeRGB色域，Rec.2020,广色域都是些啥？一文读懂 - 知乎</figcaption></figure><h4 id="几个误区">几个误区</h4><ul><li><strong>多色域显示器</strong></li></ul><p>有的专业显示器会支持不同的色域，因为广色域显示器的作用范围往往都超越sRGB，但是如果是用广义显示器来显示sRGB的话，可能会产生一些偏色与失真，即便它的色彩极为饱和。于是就要进行一些色域的缩限，就是在平常情况下将其缩放在sRGB的范围内，来保证图片不会失真。具备色域限缩功能的显示器都会特别标注支持多种色域，让用户在日常使用中使用sRGB色域显示，而在摄影修图或者印刷校样的时候，可以切换AdobeRGB色域显示，确保色彩的可靠性。</p><ul><li><strong>色域覆盖超100%</strong></li></ul><p>比如说厂家发布的新眼球，然后将那个色域标注为比如130%，即超出sRGB范围，达到了标准面积的130%，但是这有可能有一个雷点就是，他可能会利用色欲域的偏移来达到一个指标的优化，实际上的图片是失真了的。</p><ul><li><strong>72%的NTSC</strong></li></ul><p>有很多无良厂家和媒体给出过这么一个等式：72% NTSC = 100sRGB.并且以此来宣传自己的屏幕很优秀，能够用于专业的工作，不比当年 MacBook Pro100% sRGB 差，理由就是用 NTSC 测试的话，当年的 MacBook Pro 屏幕也是 72%NTSC色域。这实际上是利用了消费者认知的一个信息差，想以此来标榜自己的显示器，需要擦亮眼睛。</p><h3 id="色深">色深</h3><p>色深色深即色彩深度，色彩深度是计算机图形学领域表示在位图或者视频帧缓冲区中储存1像素的颜色所用的位数，它也称为位/像素（bpp）。色彩深度越高，可用的颜色就越多，过渡会更加丝滑。</p><p>色深表示显示器对单色通道的灰阶划分精度，8 bit 对应每通道 256级灰阶（RGB 三通道共 1670 万色），10 bit 则为 1024 级（10.7亿色）。更高的色深能减少色彩断层现象，尤其在渐变场景（如天空、阴影过渡）中表现更自然。</p><p>目前主流显示器多采用 8 bit 原生色深，部分 IPS 面板通过FRC（帧率控制） 技术抖动模拟 10 bit 效果（标注为 8 bit+FRC）。原生 10bit 面板成本较高，多见于专业设计屏或高端 OLED 屏幕。大部分 8 bit够了。</p><p><strong>注意</strong>：某些厂商会模糊标注“10.7亿色”，实际可能为抖动实现。</p><h3 id="色准">色准</h3><p>色准是衡量颜色「准确度」的指标，简单来说，就是让显示器显示什么颜色，就是什么颜色。一般用ΔE（显示颜色与标准颜色之间的偏差）来表示（数值越小越好）。</p><p>衡量参数为Delta E（△E），数值越小，色彩还原越好。</p><ul><li>△E ≤ 1：人眼几乎无法察觉色差，适合专业摄影、印刷校色。</li><li>△E ≤ 2：满足设计、影视后期等创作需求。</li><li>△E &gt; 3：肉眼可见偏色，需通过校色仪校准。</li></ul><p>专业显示器出厂前会进行校色并附赠 <strong>ΔE报告</strong>，部分型号支持硬件级校准（如 LUT写入）。日常使用中，环境光变化与面板老化可能导致色准偏移，建议定期使用校色仪（如Datacolor SpyderX、X-Rite i1Display Pro）重新校准。校色时需注意：</p><ul><li><p>预热显示器 30 分钟以上。</p></li><li><p>关闭动态对比度与 HDR 功能。</p></li><li><p>选择与工作流匹配的色域。</p></li></ul><h3 id="hdr">HDR</h3><p>这个概念这几年应该算是比较火，全称High-DynamicRange，高动态范围成像，简单的来说就是开了看着会更爽，可以在认证处看一下是什么级别。</p><p>它可以通过提升显示器的<strong>亮度范围</strong>和<strong>对比度</strong>，还原更接近真实世界的明暗细节与色彩层次。与传统SDR（标准动态范围）相比，HDR能同时保留高光部分的纹理（如阳光下的云层）和暗部场景的细节（如阴影中的岩石），显著增强画面立体感与沉浸感。</p><h4 id="标准">标准</h4><p>业界暂无统一标准。</p><ul><li><strong>HDR10</strong>：基础标准，使用静态元数据（全片统一亮度参数），支持10 bit 色深，峰值亮度通常要求 ≥1000尼特，广泛适用于游戏、影视内容。</li><li><strong>HDR10+</strong>：升级版，引入动态元数据（逐帧优化亮度），兼容性较强。</li><li><strong>Dolby Vision</strong>：杜比实验室推出的高端标准，支持 12 bit色深与动态元数据，对硬件要求更高，画面表现更细腻，多用于高端电视与流媒体（如Netflix）。</li><li><strong>DisplayHDR</strong>：由 VESA 基于HDR10制定的显示器认证体系，分多个等级（如 DisplayHDR400/600/1000），标注峰值亮度、色域覆盖等参数（例：DisplayHDR 1000 需满足1000 尼特峰值亮度、99% DCI-P3 色域）。</li></ul><h4 id="注意">注意</h4><ul><li>部分显示器仅支持 HDR 信号输入，但亮度不足或缺乏分区背光，实际效果与SDR 差异不大。</li><li>DR 对暗室环境更友好，强光下高亮优势难以体现。</li><li>需片源、播放设备、显示器均支持 HDR 才能生效，缺一不可。</li><li>HDR对游戏不是很必要，游戏在信号源上就有先天优势。</li><li>当我们看到一个电视机或者显示器，宣传自己支持HDR的时候，会有以下两种可能性。第一，电视机或显示器从显示驱动芯片到显示面板都支持HDR标准。第二，电视机或显示器的显示驱动芯片支持HDR解码，但是显示面板并不一定能够通过HDR认证。例如SDR的标准是100nit，HDR认证的最低标准是400nit，介于100nit和400nit之间显示面板，通过显示驱动芯片对HDR的支持，可以提供比SDR高的多的动态范围，让用户体验到接近HDR的视觉体验。</li></ul><h3 id="重量">重量</h3><p>便携屏要考虑一下，防止抱着一块砖到处转，这个就比较看个人了。</p><h3 id="支架与挂壁支持">支架与挂壁支持</h3><p>详见自己的需求，这里略。</p><h2 id="工具汇总">工具汇总</h2><ul><li>对比度：<ahref="https://www.pingmudiy.com/screentest/#welcome">烧屏网</a>。</li><li>DisplayHDR小软件，自己找一找吧。</li></ul><h2 id="参考">参考</h2><p><ahref="https://cn.windows-office.net/?p=10479">什么是屏幕亮度尼特？您需要多少尼特？</a></p><p><ahref="https://zhuanlan.zhihu.com/p/606290120">【科普】电视显示器色域科普：sRGB，DCI-P3，AdobeRGB色域，Rec.2020,广色域都是些啥？一文读懂 - 知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/562781181">科普百篇系列（155）液晶彩色显示屏的构造原理 - 知乎</a></p><p><ahref="https://baike.baidu.com/item/光的偏振/1277912">光的偏振_百度百科</a></p><p><ahref="https://www.bilibili.com/video/BV1Me4y1k72b?spm_id_from=333.788.videopod.sections&amp;vd_source=35efa20a3dc2d6382913a782f2bf8ad8">【硬核科普】一个视频带你了解LCDOLED QLED mini-LED等显示技术的区别_哔哩哔哩_bilibili</a></p><p><ahref="https://www.zhihu.com/question/407170937">流明，尼特，勒克斯，烛光，坎德拉有什么区别有什么关系啊？怎么使用？为啥屏幕只用尼特来计量？- 知乎</a></p><p><ahref="https://blog.csdn.net/didi_ya/article/details/122241863">【科普】显示器VGA、DVI、HDMI、DP等各种接口详细科普_显示器接口-CSDN博客</a></p><p><ahref="https://zhuanlan.zhihu.com/p/440706026">一文搞懂，显示器接口的VGA、HDMI、DVI 和 DisplayPort 、雷霹、Type-C接口有什么区别？ -知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/70673207">显示器接口都有哪些？- 知乎</a></p><p><ahref="https://zhuanlan.zhihu.com/p/394352018">【显示器科普知识6】液晶显示器技术参数：对比度- 知乎</a></p><p><ahref="https://zhuanlan.zhihu.com/p/398281787">硬核科普│什么是HDR？一文秒懂为何大家都向往HDR模式！- 知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/70383462">基础px、pt、ppi的知识- 知乎</a></p><p><a href="https://zh.wikipedia.org/wiki/色彩深度">色彩深度 -维基百科，自由的百科全书</a></p><p><ahref="https://zhuanlan.zhihu.com/p/259139655#:~:text=色深代表着画面颜色的「细腻程度」，也以简单理解为颜色数量的多少。%20并且，数值越大就越细腻，同时色彩过渡就更加平滑自然。%20目前主流显示器的色深一般是6%20bit、8bit和10bit。%20具体而言，色深8bit指的是红（R）、绿（G）、蓝（B）这三原色，在显示颜色时各有2的%208次方种（即%20256%20种）。,种不同的绿、256种不同的蓝，这三种原色组合起来，总的颜色数量就是256×256×256%3D16.7%20百万。%20以此类推，10bit的色数就是%2010.7%20亿，而6bit的色数是%200.26%20百万。%20由此可见6bit的色数是相对偏少的，因此色彩显示效果就比较差。">色深、色域、色准、HDR、防撕裂你都懂吗？- 知乎</a></p>]]></content>
    
    
    <categories>
      
      <category>电脑硬件</category>
      
      <category>选购</category>
      
    </categories>
    
    
    <tags>
      
      <tag>硬件</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于人体关键点识别的摔倒检测算法</title>
    <link href="/2025/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/2025-04-05-%E5%9F%BA%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9%E8%AF%86%E5%88%AB%E7%9A%84%E6%91%94%E5%80%92%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/"/>
    <url>/2025/04/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5/2025-04-05-%E5%9F%BA%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9%E8%AF%86%E5%88%AB%E7%9A%84%E6%91%94%E5%80%92%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1id="基于人体关键点识别的摔倒检测算法">基于人体关键点识别的摔倒检测算法</h1><p>最开始我们做跌倒检测的时候用的是传统的单纯用yolo等目标检测算法，但是在交流中我们也发现一个问题，就是传统的目标检测难以区分摔倒与跑跳坐下蹲下等动作之间的区别，即难以实现类间分类，然后就是一拍脑门想起来了可以基于人体关键点识别来做跌倒检测，和其他的动作区分开来，而且这个方案已经比较成熟了，详细可见下面的参考文献。在这里先做一下算法的调研。之后等这个项目结题后再进行具体模型的分享。</p><h2 id="现存算法">现存算法</h2><h3 id="传统时空分析">传统时空分析</h3><p>通过实时跟踪人体骨骼关键点（如头、肩、膝盖、脚踝等），分析这些点的位置变化规律来判断是否跌倒。优点是直观和解释性强，适合实时的检测；缺点为数据缺失时可能出错，这个在鲁棒性上可以做一些工作。</p><pre><code class=" mermaid">flowchart LR    A[视频/图像输入] --&gt; B[关键点提取]    B --&gt; C[动态特征计算]    C --&gt; D[分类决策]    D --&gt; E&#123;&#123;跌倒判定结果&#125;&#125;    %% 关键点提取模块    subgraph keypoint_extract [关键点提取]        direction TB        B1[OpenPose] --&gt;|输出关键点| B2[BlazePose]    end    %% 动态特征计算模块    subgraph feature_cal [动态特征计算]        direction TB        C1[质心高度&lt;br&gt;（头部/躯干骤降）]:::feature        C2[关节角度&lt;br&gt;（躯干-地面夹角）]:::feature        C3[运动速度&lt;br&gt;（垂直加速度突增）]:::feature    end    %% 分类决策模块    subgraph decision [分类决策]        direction TB        D1[阈值法]:::decision --&gt;|&quot;高度 &lt; 阈值?&quot;| D1a[报警]        D2[机器学习模型]:::decision --&gt; D2a[SVM] &amp; D2b[随机森林]    end    %% 应用样式    class A input;    class B,B1,B2 process;    class C,C1,C2,C3 feature;    class D,D1,D2 decision;    class E result;</code></pre><ul><li><p>关键点提取：</p><p>人体关键点检测模型（如OpenPose、BlazePose）从视频中抓取人体关键点坐标。</p></li><li><p>动态特征计算：</p></li></ul><p>​ 质心高度：头部或躯干关键点的高度突然下降可能是跌倒信号。</p><p>​关节角度：如肩膀-膝盖连线与地面的夹角急剧变小（例如从站立时的90°到跌倒后的接近0°）。</p><p>​运动速度：头部或躯干在垂直方向的加速度骤增（例如跌倒时的“失重”瞬间）。</p><ul><li>分类决策：</li></ul><p>​ 手动调阈值：设定特征阈值（如质心高度低于膝盖时判定跌倒）。</p><p>​ 机器学习模型：用SVM或随机森林基于特征组合分类。直接上机器学习算了。</p><hr /><h3 id="深度学习时序建模">深度学习时序建模</h3><p>算力和数据要求比较高。</p><ul><li>LSTM：分析关键点序列的时序变化，捕捉“跌倒前-跌倒中-跌倒后”的动态过程。<ul><li>例如，专利CN112163564A将人体分为头、躯干、腿三区域，用LSTM记忆动作轨迹，预判跌倒风险。<br /></li></ul></li><li>3DCNN：直接输入视频片段，同时提取空间（姿态）和时间（动作）特征（勉强算吧，做出来了也是勾八）。<br /></li><li>图卷积网络（GCN）：将人体骨骼视为图结构，分析关节间的关联性，适合处理复杂动作。这个网络还不咋熟，到时候再学一下。</li></ul><hr /><h3id="基于clahe与形态学的目标骨架提取"><strong>基于CLAHE与形态学的目标骨架提取</strong></h3><p>这个主要是骨架提取的，不需要深度学习模型，计算资源需求不高，但是可能在复杂背景下的鲁棒性不足。</p><ul><li><strong>图像增强</strong>：使用限制对比度自适应直方图均衡化（CLAHE）提升图像对比度，突出人体轮廓。</li><li><strong>骨架提取</strong>：<ol type="1"><li>背景差分法分离运动目标，形态学闭运算填充空洞。</li><li>边缘检测（如Sobel算子）提取人体轮廓，中值滤波去噪。</li><li>细化算法生成人体骨架，提取近似关键点（如头部、四肢末端）。</li></ol></li></ul><hr /><h3 id="拓展">拓展</h3><p><strong>遮挡问题</strong>：</p><p>使用时空滤波（如滑动窗口滤波）填补遮挡导致的关键点缺失。</p><p><strong>实时性优化</strong>：</p><ul><li><p>采用模型蒸馏技术，将大模型的知识迁移到小模型。</p></li><li><p>通过OpenVINO™等工具将模型部署到边缘设备，减少云端依赖。</p></li></ul><p><strong>鲁棒性优化（这个robust的翻译见一次骂一次）</strong></p><ul><li>多传感器融合，比如加个手环上面整点加速计之类的，但是这个成本直接干上去了</li></ul>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>实践</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>标准分数-Z-Score</title>
    <link href="/2025/04/04/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E5%B0%8F%E5%B7%A5%E5%85%B7/2025-04-04-%E6%A0%87%E5%87%86%E5%88%86%E6%95%B0-Z-Score/"/>
    <url>/2025/04/04/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E5%B0%8F%E5%B7%A5%E5%85%B7/2025-04-04-%E6%A0%87%E5%87%86%E5%88%86%E6%95%B0-Z-Score/</url>
    
    <content type="html"><![CDATA[<h1 id="z-score">Z-Score</h1><h2 id="一简介">一、简介</h2><p>一种衡量数据点相对于总体均值的位置的分数，可将不同量纲或不同分布的数据转化到同一标准下（标准正态分布），方便比较。正值表示数据大小在均值之上，负值则在均值之下。</p><hr /><h2 id="二公式">二、公式</h2><p><span class="math display">\[\frac{X-\bar{X} }{s}\]</span></p><p>及均值差除以标准差。matlab中可以直接使用<code>zscore()</code>函数。</p><hr /><h2 id="三应用">三、应用</h2><ul><li><strong>异常值检测</strong> zscore 可用于判断异常值（Outlier）：通常设置阈值（如 2 或 3），如果一个数据的 |z|值大于该阈值，则该数据可能为异常值。 例如：若 |z| &gt;3，则通常认为该数据偏离均值过远，可以进一步检查是否为异常情况。这个可以从正态分布中类比出来，约68% 的数据落在 [-1, 1] 范围内，约 95% 的数据落在 [-2, 2] 范围内，约99.7% 的数据落在 [-3, 3] 范围内</li><li><strong>数据标准化</strong> 在机器学习或数据挖掘中，通过对数据进行zscore 标准化，可以消除量纲影响，利于建模。</li><li><strong>比较不同数据集</strong> 由于 zscore消除了原始数据的均值和标准差的影响，所以可以将来自不同数据集的数值进行直接比较。</li></ul><hr /><h2 id="四局限性">四、局限性</h2><p>zscore对于数据的分布有一定的要求，越靠近正态分布的情况下越好。</p><p>zscore消除了数据原有的实际意义，缺乏可解释性，结果只能用于比较。如果需要解释数据的真实意义，还需要还原原值。</p><hr /><h2 id="参考资料">参考资料</h2><p><ahref="https://blog.csdn.net/qy20115549/article/details/53117742">数据标准化方法z-score讲解(matlab)_matlabzscore函数-CSDN博客</a></p><p><ahref="https://baike.baidu.com/item/Z分数/8268473">Z分数_百度百科</a></p><p><a href="https://zhuanlan.zhihu.com/p/69074703">数据标准化（一） -Z-Score标准化 - 知乎</a></p><p><ahref="https://blog.csdn.net/qq_39482438/article/details/110873346">Z-Score：定义，公式和 数据标准化-CSDN博客</a></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>数学建模</category>
      
      <category>小工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优劣解距离法-TOPSIS</title>
    <link href="/2025/04/01/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-04-01-%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95-TOPSIS/"/>
    <url>/2025/04/01/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-04-01-%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95-TOPSIS/</url>
    
    <content type="html"><![CDATA[<h1 id="优劣解距离法-topsis">优劣解距离法-TOPSIS</h1><h2 id="一建模">一、建模</h2><p>TOPSIS进行建模，大致分为以下四个步骤：</p><ol type="1"><li>将原始矩阵正向化</li><li>将正向化矩阵标准化</li><li>计算得分并归一化</li></ol><p>个人感觉跟熵权法是有很多的相通之处的，二者应该可以相互混着用。</p><h3 id="矩阵正向化">矩阵正向化</h3><p>在生活中，常见的指标有四种：</p><table><thead><tr><th>指标名称</th><th>指标特点</th><th>例子</th></tr></thead><tbody><tr><td>极大型（效益型）指标</td><td>越大（多）越好</td><td>成绩、GDP增速、企业利润</td></tr><tr><td>极小型（成本型）指标</td><td>越小（少）越好</td><td>费用、坏品率、污染程度</td></tr><tr><td>中间型指标</td><td>越接近某个值越好</td><td>水质量评估时的PH值</td></tr><tr><td>区间型指标</td><td>落在某个区间最好</td><td>体温、水中植物性营养物量</td></tr></tbody></table><p>那么，在TOPSIS方法中，就是要将所有指标进行统一正向化，即统一转化为极大型指标。那么就需要极小型、中间型以及区间型的指标进行转化为极大型指标。</p><p><strong>极小型转极大型</strong>： <span class="math display">\[x_{new}= \frac{\max - x_i}{\max - \min}\]</span> 如果所有的元素均为正数，那么也可以使用：</p><p><span class="math display">\[x_{new}=\frac{1}{x_i}\]</span></p><p><strong>中间型转极大型：</strong>指标值既不要太大也不要太小，取某特定值最好（如水质量评估PH值）。设<spanclass="math inline">\(\{x_i\}\)</span>是一组中间型指标序列，且最佳的数据为<span class="math inline">\(x_{best}\)</span>，那么正向化的公式如下：<span class="math display">\[M = \max\{|x_i - x_{best}|\}\]</span> <span class="math display">\[\bar{x}_i = 1 - \frac{|x_i - x_{best}|}{M}\]</span> <strong>区间型指标：</strong>指标值落在某个区间内最好，例如人的体温在36°～37°这个区间比较好。设<spanclass="math inline">\(\{x_i\}\)</span>是一组区间型指标序列，且最佳的区间为<spanclass="math inline">\([a,b]\)</span>，那么正向化的公式如下： <spanclass="math display">\[M = \max\{a - \min\{x_i\}, \max\{x_i\} - b\},\ \tilde{x}_i =\begin{cases}1 - \frac{a - x_i}{M}, &amp; x_i &lt; a \\1, &amp; a \leq x_i \leq b \\1 - \frac{x_i - b}{M}, &amp; x_i &gt; b\end{cases}\]</span> 实际上这里跟熵权法里面讲过的那个正向化基本上一模一样，具体见<ahref="熵权法(EWM).md">熵权法</a></p><h3 id="矩阵标准化">矩阵标准化</h3><p>标准化的目的就是消除不同量纲的影响。</p><p>假设有n个要评价的对象，m个评价指标（已经正向化了）构成的正向化矩阵如下：</p><p><span class="math display">\[X =\begin{bmatrix}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1m} \\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2m} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nm}\end{bmatrix}\]</span> 那么对其标准化后的矩阵记为Z，Z的每一个元素：</p><p><span class="math display">\[z_{ij} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^{n}x_{ij}^2}}\]</span> 标准化矩阵Z：</p><p><span class="math display">\[Z =\begin{bmatrix}z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1m} \\z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2m} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\z_{n1} &amp; z_{n2} &amp; \cdots &amp; z_{nm}\end{bmatrix}\]</span>注意：标准化的方法不唯一，但目的都是为了去量纲。比如熵权法那一章就还介绍过存在负数时直接上</p><p><span class="math display">\[z_{ij} = \frac{x_{ij} - \min\{x_{1j}, x_{2j}, \ldots,x_{nj}\}}{\max\{x_{1j}, x_{2j}, \ldots, x_{nj}\} - \min\{x_{1j}, x_{2j},\ldots, x_{nj}\}}\]</span></p><h3 id="计分归一化">计分归一化</h3><p>最大值：</p><p><span class="math display">\[Z^+ = (\max\{z_{11},z_{21},\cdots,z_{n1}\},\max\{z_{12},z_{22},\cdots,z_{n2}\}, \cdots,\max\{z_{1m},z_{2m},\cdots,z_{nm}\})\]</span> 最小值： <span class="math display">\[Z^- = (\min\{z_{11},z_{21},\cdots,z_{n1}\},\min\{z_{12},z_{22},\cdots,z_{n2}\}, \cdots,\min\{z_{1m},z_{2m},\cdots,z_{nm}\})\]</span> 定义第 <span class="math inline">\(i(i =1,2,…,n)\)</span>个评价对象与最大值的距离： <spanclass="math display">\[D_i^+ = \sqrt{\sum_{j=1}^{m}(Z_j^+ - z_{ij})^2}\]</span> 定义第 <span class="math inline">\(i(i = 1,2,…,n)\)</span>个评价对象与最小值的距离：</p><p><span class="math display">\[D_i^- = \sqrt{\sum_{j=1}^{m}(Z_j^- - z_{ij})^2}\]</span> 那么，我们可以计算得出第 <span class="math inline">\(i(i =1,2,…,n)\)</span> 个评价对象未归一化的得分：</p><p><span class="math display">\[S_i = \frac{D_i^-}{D_i^+ + D_i^-}\]</span> 很明显 <span class="math inline">\(0 \leq S_i \leq1\)</span>，且 <span class="math inline">\(S_i\)</span> 越大 <spanclass="math inline">\(D_i^+\)</span> 越小，即越接近最大值。</p><h2 id="二扩展">二、扩展</h2><h4 id="权重结合">权重结合</h4><p>上述过程默认了各项指标的权重相同，但在实际的评价中指标都是有各自的权重，因此应该用权重对公式进行修正，修正后的公式如下，ω代表权重。<span class="math display">\[D_i^+ = \sqrt{\sum_{j=1}^{m}\omega_j(Z_j^+ - z_{ij})^2},\ D_i^- =\sqrt{\sum_{j=1}^{m}\omega_j(Z_j^- - z_{ij})^2}\]</span> 这里就可以直接上熵权法，当然，专家瞎打分也不是不行。</p><h4 id="本质推导">本质推导</h4><p>公式：</p><p><span class="math display">\[S_i = \frac{D_i^-}{D_i^+ + D_i^-}\]</span> 实际上是：</p><p><span class="math display">\[\frac{x - \min}{\max - \min} \rightarrow \frac{x - \min}{(\max - x) + (x- \min)} \rightarrow \frac{x与最小值的距离}{x与最大值的距离 +x与最小值的距离}\]</span> <span class="math inline">\(D_i^+\)</span>与<spanclass="math inline">\(D_i^-\)</span>实际上是<strong>欧氏距离</strong>，这个的本质思想和熵权法的实际上是有很多的相同之处的。</p><h2 id="参考">参考</h2><p><ahref="https://blog.csdn.net/weixin_43819566/article/details/112342602">清风数学建模学习笔记——TOPSIS法（优劣解距离法）-CSDN博客</a></p><p><ahref="https://zhuanlan.zhihu.com/p/564302492">数学建模——常考评价类模型介绍- 知乎</a></p><p><ahref="https://zhuanlan.zhihu.com/p/266689519">TOPSIS(逼近理想解)算法原理详解与代码实现- 知乎</a></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>数学建模</category>
      
      <category>评价类模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模糊综合评价法-FCE</title>
    <link href="/2025/04/01/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-04-01-%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7%E6%B3%95-FCE/"/>
    <url>/2025/04/01/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-04-01-%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7%E6%B3%95-FCE/</url>
    
    <content type="html"><![CDATA[<h1 id="模糊综合评价法">模糊综合评价法</h1><p>量分为 <strong>确定性</strong> 和 <strong>不确定性</strong>：</p><ul><li><strong>确定性</strong>：经典数学（几何、代数）</li><li><strong>不确定性</strong>：<ol type="1"><li>随机性（概率论、随机过程）如掷筛子，观看那一面向上，这种现象的规律性靠概率统计去刻画。</li><li>灰性（灰色系统）</li><li>模糊性（模糊数学）如“今天天气很热”、“小伙子很帅”等，靠模糊数学去刻画。</li></ol></li></ul><p>而模糊综合评价法是一种基于 <strong>模糊数学</strong> 的综合<strong>评价方法</strong>。该综合评价法根据模糊数学的<strong>隶属度理论</strong>把定性评价转化为定量评价，即用模糊数学对受到多种因素制约的事物或对象做出一个总体的评价。它具有结果清晰，系统性强的特点，能较好地解决模糊的、难以量化的问题，适合各种<strong>非确定性问题的解决</strong>。</p><h2 id="经典集合和模糊集合的基本概念">经典集合和模糊集合的基本概念</h2><h3 id="经典集合classical-set和特征函数">经典集合（classicalset）和特征函数</h3><ul><li><strong>经典集合</strong>：具有相同属性的事物的集体，例如：颜色、性别、手机品牌等、自然数集。</li><li>集合的基本属性：互斥、确定，就是从高中以来我们一直所认为的集合。</li></ul><h3 id="模糊集合fuzzy-set和隶属函数">模糊集合（fuzzyset）和隶属函数</h3><ul><li><p><strong>模糊集合</strong>：用来描述模糊性概念的集合。（帅、高、白、年轻…）</p></li><li><p>与经典集合相比，模糊集合承认亦此亦彼，即不具有确定性和互斥性，而我们很多情况下都是这样的集合，比如25年第一次校赛B题。</p></li><li><p>数学中对于模糊集合的刻画：<strong>隶属函数</strong></p><p><span class="math display">\[\mathbf{u_A: U \rightarrow [0,1]}\]</span> 其中注意与 <span class="math inline">\(\{0, 1\}\)</span>的区别，<span class="math inline">\(\{0, 1\}\)</span>只有两种可能，<span class="math inline">\([0, 1]\)</span>有无数种可能。</p></li></ul><h2 id="隶属函数的确定方法">隶属函数的确定方法</h2><h3 id="模糊统计法">模糊统计法</h3><p>自己捏数或者放问卷。</p><h3 id="借助已有的客观尺度">借助已有的客观尺度</h3><p>比如用恩格尔系数描述家庭贫富状况。</p><h3 id="指派法">指派法</h3><p><strong>根据问题的性质直接套用某些分布作为隶属函数，主观性较强。</strong></p><figure><imgsrc="D:\blogs\source\img\inblogs\533fb99c13fa8f6ca63aeb208c4bf273.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>其中，梯形分布用得较多。</p><h2 id="应用模糊综合评价">应用：模糊综合评价</h2><h3 id="评价问题概述">评价问题概述</h3><p>模糊评价问题是要解决的问题是：</p><blockquote><p>① 把论域中的对象对应评语集中的一个指定的评语。 ②将方案作为评语集并选择一个最优的方案。</p></blockquote><p>在模糊综合评价中，引入三个集合：（下面的符号和概念中的符号表示的<strong>含义不同</strong>）</p><ul><li><strong>因素集（评价指标集）</strong> <span class="math inline">\(U= \{u_1,u_2,...,u_n\}\)</span></li><li><strong>评语集（评价的结果集）</strong> <spanclass="math inline">\(V = \{v_1,v_2,..,v_m\}\)</span></li><li><strong>权重集（指标的权重）</strong> <span class="math inline">\(A= \{a_1,a_2,...,a_n\}\)</span></li></ul><h3 id="一级模糊综合评价">一级模糊综合评价</h3><ol type="1"><li><p><strong>确定因素集</strong></p><p>一级模糊评价中，n 往往较小（一般 ≤ 5）且 指标间相关性不强。</p></li><li><p><strong>确定评语集</strong></p></li><li><p><strong>第三步：确定各因素的权重</strong></p><p>确定权重的方法有很多，如：Delphi法（专家调查法，即瞎几把捏数法）、加权平均法、众人评估法。但是建议：当没有数据的时候可采取层次分析法，有数据的时候可采取熵权法。</p></li><li><p><strong>第四步：确定模糊综合评判矩阵，对每个元素做出评价</strong></p></li><li><p><strong>第五步：模糊综合评判</strong></p></li></ol><h3 id="多级模糊综合评价">多级模糊综合评价</h3><p>一级模糊综合评价是多级模糊综合评价的基础。它是将一级模糊评价因素集指标的相关性指标进行统一综合，因为上文讲过一级模糊评价的指标之间是相关性不强的，那么这样相关性指标在只考虑这一种因素的情况下是不相关的，即相对不相关。然后对每个相关性指标进行综合，求得只看这一组指标对于评语集的隶属度，此方法进行类推，最后再综合。</p><h2 id="总结">总结</h2><ol type="1"><li>隶属函数有三种确定方法，要根据实际情况来选择使用。</li><li>极大型（极小型）指标，和模糊集合的三类（偏小型、中间型、偏大型）是两种概念，注意区分。</li><li>模糊综合评价的因素指标是相关性不强的，如果有相关项较强的需要进行多级模糊综合评价，级数一般不超过3层。</li></ol>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>数学建模</category>
      
      <category>评价类模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>主成分分析-PCA</title>
    <link href="/2025/04/01/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-04-01-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA/"/>
    <url>/2025/04/01/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-04-01-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-PCA/</url>
    
    <content type="html"><![CDATA[<h1 id="主成分分析-pca">主成分分析-PCA</h1><p>主成分分析（PCA）是一种降维算法，通过将原始变量进行线性组合，生成互不相关的主成分，从而保留数据的主要信息，以此来去噪，提升数据处理速度。PCA常用于数据简化、聚类分析及解决回归中多重共线性问题，但在解释主成分时往往较为模糊，故不常用于评价模型。</p><hr /><h2 id="一简介">一、简介</h2><ul><li><strong>目标：</strong>用较少的新变量替代原来较多的变量，尽可能保留原有信息。<br /></li><li><strong>应用：</strong> 数据降维、去除噪声、提升数据处理速度。<br /></li><li><strong>适用场景</strong>：指标高度相关易于解释、可解释需求低。</li><li><strong>局限性</strong>：非线性关系处理的不好，贡献度若是不足可能会丢失细节。</li></ul><hr /><h2 id="二思想">二、思想</h2><p>假设有 <span class="math inline">\(n\)</span> 个样本和 <spanclass="math inline">\(p\)</span> 个指标，构成一个 $ n p$的样本矩阵：</p><p><span class="math display">\[X = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \\\end{bmatrix}= (x_1, x_2, \cdots, x_p)\]</span> 目标是寻找新的变量 $ z_1, z_2, , z_m $（其中 $ m p$），使其满足：</p><p><span class="math display">\[\begin{cases}z_1 = l_{11}x_1 + l_{12}x_2 + \cdots + l_{1p}x_p \\z_2 = l_{21}x_1 + l_{22}x_2 + \cdots + l_{2p}x_p \\\vdots \\z_m = l_{m1}x_1 + l_{m2}x_2 + \cdots + l_{mp}x_p \\\end{cases}\]</span> <strong>系数 $ l_{ij} $ 的确定原则：</strong></p><ol type="1"><li>不同主成分之间彼此不相关。</li><li>第一主成分是所有线性组合中方差最大的。</li><li>第二主成分在与第一主成分不相关的条件下方差最大。</li><li>依此类推，直到选出 $ m $ 个主成分。</li></ol><hr /><h2 id="三计算步骤">三、计算步骤</h2><h3 id="数据标准化">1. 数据标准化</h3><ul><li><p><strong>计算均值和标准差：</strong> <span class="math display">\[\bar{x}_j = \frac{1}{n} \sum_{i=1}^{n} x_{ij},\quad S_j =\sqrt{\frac{\sum_{i=1}^{n}(x_{ij}-\bar{x}_j)^2}{n-1}}\]</span></p></li><li><p><strong>标准化数据：</strong> <span class="math display">\[X_{ij} = \frac{x_{ij}-\bar{x}_j}{S_j}\]</span></p></li><li><p><strong>标准化后的矩阵：</strong> <span class="math display">\[X = \begin{bmatrix}X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1p} \\X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2p} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{np} \\\end{bmatrix}\]</span></p></li></ul><h3 id="计算协方差矩阵">2. 计算协方差矩阵</h3><ul><li><strong>定义：</strong> <span class="math display">\[R = \begin{bmatrix}r_{11} &amp; r_{12} &amp; \cdots &amp; r_{1p} \\r_{21} &amp; r_{22} &amp; \cdots &amp; r_{2p} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\r_{p1} &amp; r_{p2} &amp; \cdots &amp; r_{pp} \\\end{bmatrix},\quad r_{ij} = \frac{1}{n-1}\sum_{k=1}^{n} X_{ki} X_{kj}\]</span></li></ul><h3 id="求解特征值和特征向量">3. 求解特征值和特征向量</h3><ul><li><p><strong>特征值：</strong> <span class="math display">\[\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0\]</span> （且 $ R $ 为半正定矩阵）</p></li><li><p><strong>对应的特征向量：</strong> <span class="math display">\[a_1 = \begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{p1}\end{bmatrix},\quada_2 = \begin{bmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{p2}\end{bmatrix},\quad \dots,\quada_p = \begin{bmatrix} a_{1p} \\ a_{2p} \\ \vdots \\ a_{pp} \end{bmatrix}\]</span></p></li><li></li></ul><h3 id="计算贡献率和累计贡献率">4. 计算贡献率和累计贡献率</h3><ul><li><p><strong>贡献率：</strong> <span class="math display">\[\text{贡献率} = \frac{\lambda_i}{\sum_{k=1}^{p}\lambda_k}\]</span></p></li><li><p><strong>累计贡献率：</strong> <span class="math display">\[\text{累计贡献率} =\frac{\sum_{k=1}^{i}\lambda_k}{\sum_{k=1}^{p}\lambda_k}\quad(i=1,2,\dots,p)\]</span></p></li><li></li></ul><h3 id="写出主成分">5. 写出主成分</h3><p>一般选择累计贡献率超过 80% 的前 ( m ) 个主成分：</p><p><span class="math display">\[F_i = a_{1i}X_1 + a_{2i}X_2 + \cdots + a_{pi}X_p,\quad (i=1,2,\dots,m)\]</span></p><h3 id="主成分的解释">6. 主成分的解释</h3><p>根据各指标在主成分中的载荷大小判断该主成分代表的含义，载荷越大的指标对该主成分的影响越大。</p><hr /><h2 id="四扩展">四、扩展</h2><ol type="1"><li><strong>聚类分析：</strong>可通过降维简化自变量，便于图形展示。</li><li><strong>回归分析：</strong> 可用于缓解多重共线性问题。</li><li><strong>因子分析：</strong> PCA实际上是因子分析的一种特例，但因子分析在解释方面更具优势，建议多用因子分析。</li></ol><hr /><h2 id="参考资料">参考资料</h2><p><ahref="https://blog.csdn.net/weixin_43819566/article/details/113800120">清风数学建模学习笔记——主成分分析(PCA)原理详解及案例分析_x10为生均教育经费对以上指标数据做主成分分析，并提取主成分-CSDN博客</a></p><p><ahref="https://developer.baidu.com/article/details/3015610">数学建模中的主成分分析(PCA)详解与应用-百度开发者中心</a></p><p><ahref="https://zhuanlan.zhihu.com/p/677797684">【数模百科】一文讲清楚主成分分析PCA（附python代码）- 知乎</a></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>数学建模</category>
      
      <category>评价类模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>皮尔逊相关系数-PEARSON</title>
    <link href="/2025/03/31/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E5%B0%8F%E5%B7%A5%E5%85%B7/2025-03-31-%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    <url>/2025/03/31/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E5%B0%8F%E5%B7%A5%E5%85%B7/2025-03-31-%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="皮尔逊相关系数">皮尔逊相关系数</h1><p>方差描述的是单个变量的离散程度，协方差给方差的一个 <spanclass="math inline">\((X - E[X])^2\)</span> 变成 <spanclass="math inline">\((X - E[X])(Y - E[Y])\)</span>，就能衡量的是两个变量偏离各自均值后乘积的平均值，反映了两变量共同变化的趋势。但由于协方差受变量的单位影响，难以直接比较，所以我们就要将其无量纲化，除上各自的标准差就好。</p><h2 id="定义">定义</h2><p>皮尔逊相关系数（通常称为<strong>线性相关系数</strong>）用于刻画变量 X与 Y 之间的线性关系。其定义为： <span class="math display">\[\rho_{X, Y} = \frac{\operatorname{cov}(X, Y)}{\sigma_X \sigma_Y} =\frac{E\left[(X - E[X])(Y - E[Y])\right]}{\sigma_X \sigma_Y} =\frac{E(XY) - E(X)E(Y)}{\sqrt{E\left(X^2\right) - [E(X)]^2}\sqrt{E\left(Y^2\right) - [E(Y)]^2}}\]</span></p><blockquote><p><strong>注：</strong> 当 X 与 Y存在其他非线性关系时，皮尔逊相关系数无法正确反映两者之间的关系。</p></blockquote><hr /><h2 id="相关系数的性质">相关系数的性质</h2><ol type="1"><li><p><strong>独立性与不相关性：</strong></p><p>如果 X 和 Y 独立，则 <span class="math inline">\(\rho_{X,Y} =0\)</span>；但反过来，<span class="math inline">\(\rho_{X,Y} =0\)</span> 不一定说明 X 与 Y 独立，除非二者服从二维正态分布。</p></li><li><p><strong>取值范围：</strong></p></li></ol><p><span class="math display">\[-1 \leq \rho_{X,Y} \leq 1\]</span></p><hr /><h2 id="两组指标相关系数的计算">两组指标相关系数的计算</h2><h3 id="均值">均值：</h3><p><span class="math display">\[\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i,\quad \bar{Y} =\frac{1}{n}\sum_{i=1}^{n} Y_i\]</span></p><h3 id="方差无偏估计">方差（无偏估计）：</h3><p><span class="math display">\[\sigma_X^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2,\quad\sigma_Y^2 = \frac{1}{n-1}\sum_{i=1}^{n}(Y_i - \bar{Y})^2\]</span></p><h3 id="样本协方差">样本协方差：</h3><p><span class="math display">\[\operatorname{cov}(X,Y) = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})(Y_i- \bar{Y})\]</span></p><h3 id="相关系数">相关系数：</h3><p><span class="math display">\[r = \frac{\sum_{i=1}^{n}(X_i - \bar{X})(Y_i -\bar{Y})}{\sqrt{\sum_{i=1}^{n}(X_i - \bar{X})^2}\sqrt{\sum_{i=1}^{n}(Y_i - \bar{Y})^2}}\]</span></p><blockquote><p><strong>说明：</strong>分母中用的是各自的标准差，目的是将协方差无量纲化，从而消除不同变量之间单位和量级的影响，使得计算结果统一在([-1, 1]) 之间。</p></blockquote><hr /><h2 id="判断标准">判断标准</h2><p>根据计算得出的相关系数 (r)的绝对值，可以判断两个变量之间的相关性强弱：</p><table><thead><tr><th>|r|</th><th>相关强度</th></tr></thead><tbody><tr><td>0.8-1.0</td><td>极强相关</td></tr><tr><td>0.6-0.8</td><td>强相关</td></tr><tr><td>0.4-0.6</td><td>中等程度相关</td></tr><tr><td>0.2-0.4</td><td>弱相关</td></tr><tr><td>0.0-0.2</td><td>极弱相关或无相关</td></tr></tbody></table><hr /><h2 id="其它参考资源">其它参考资源</h2><ul><li><a href="#">带你深入理解期望、方差、协方差的含义</a></li><li><a href="https://www.zhihu.com/question/19734616">知乎：如何理解Pearson 相关系数</a></li><li><ahref="https://blog.csdn.net/MoreAction_/article/details/106195689">相关系数——皮尔逊相关系数的公式及其理解_皮尔逊相关系数公式-CSDN博客</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>数学建模</category>
      
      <category>小工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>熵权法-EWM</title>
    <link href="/2025/03/31/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-03-31-%E7%86%B5%E6%9D%83%E6%B3%95(EWM)/"/>
    <url>/2025/03/31/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B/2025-03-31-%E7%86%B5%E6%9D%83%E6%B3%95(EWM)/</url>
    
    <content type="html"><![CDATA[<h1 id="熵权法-ewm">熵权法-EWM</h1><h2 id="背景">背景</h2><p>一种常见于数学建模的更为客观的分配权重的方法，因为层次分析法主观性太强，专家打分实际上就是自己瞎几把打，故引入熵权法。</p><h2 id="思路">思路</h2><p>对于一组数据指标，如果它的方差越大，就说明这个数据的相似度越小，就是它所贡献度就越大。极端来看，就如果对于某项指标来讲，他的所有的值是一样的，那么它的相似度是最大的，基于这个指标来进行判别的权重可以直接设为0。熵是不确定度的一种度量，不确定性越大，熵就越大，信息量就越大，它的对最终决策的影响权重应该就设置得更高一些。</p><p>举个清风数学建模的例子，小张和小王是两个高中生，小张学习好回回期末考满分，小王学习不好考试常常不及格。在一次考试中，小张还是考了满分，而小王也考了满分。那就很不一样了，小王这里包含的信息就非常大，所对应的权重也就高一些。</p><h2 id="定义">定义</h2><ul><li><p><strong>信息量</strong></p><p>定义信息量为：<br /><span class="math display">\[I(x) = -\ln(p(x))\]</span></p></li><li><p><strong>信息熵</strong> 定义信息熵为各信息量的期望值： <spanclass="math display">\[H(X) = -\sum_{i=1}^{n} p(x_i)\ln(p(x_i))\]</span> 当所有事件均等时，熵达到最大值：<br /><span class="math display">\[H(X) = \ln(n)\]</span></p></li></ul><hr /><h2 id="计算步骤">计算步骤</h2><p>熵权法主要包括以下三个步骤：</p><h3 id="数据标准化">数据标准化</h3><p>假设有个对象和个指标，原始数据构成矩阵（要先正向化）：</p><p><span class="math display">\[X = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1m} \\x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2m} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nm}\end{bmatrix}\]</span></p><h4 id="正向化处理">正向化处理</h4><ul><li><strong>要求</strong>：后续计算概率时，每个元素必须为非负值。<br /></li><li><strong>处理</strong>：如果其中存在负数，则需采用特定方法将其标准化到[0,1] 区间。</li></ul><table><thead><tr><th>指标名称</th><th>指标特点</th><th>例子</th></tr></thead><tbody><tr><td>极大型指标（正向指标）</td><td>越大越好</td><td>成绩、利润、GDP增速</td></tr><tr><td>极小型指标（负向指标）</td><td>越小越好</td><td>花费、污染程度、失业率</td></tr><tr><td>中间型指标</td><td>越接近某个值越好</td><td>水质量评估时的PH值</td></tr><tr><td>区间型指标</td><td>落在某个区间最好</td><td>体温、水中某物质含量</td></tr></tbody></table><ul><li>极小型转极大型：</li></ul><p><span class="math display">\[x_i = \max\{x_1, x_2, ..., x_i\} - x_i\]</span></p><p>注：若所有元素均为正数，可以直接取倒数。</p><ul><li>中间行转极大型：</li></ul><p>中间型指标序列，且最佳数值为 <spanclass="math inline">\(x_{best}\)</span>，<spanclass="math inline">\(x_{new}\)</span>为正向化之后的极大型指标，正向化公式：</p><p><span class="math display">\[  M = \max\{|x_i - x_{best}|\}\]</span></p><p><span class="math display">\[x_{new} = 1 - \frac{|x_i - x_{best}|}{M}\]</span></p><ul><li>区间型转极大型:</li></ul><p>区间型指标序列，且最佳数值为 <spanclass="math inline">\(x_{best}\)</span>，<spanclass="math inline">\(x_{new}\)</span>为正向化之后的极大型指标，正向化公式： <span class="math display">\[M = \max\{a - \min\{x_i\}, \max\{x_i\} - b\}\]</span></p><p><span class="math display">\[x_{new} =\begin{cases}1 - \frac{a - x}{M}, &amp; x &lt; a \\1, &amp; a \leq x \leq b \\1 - \frac{x - b}{M}, &amp; x &gt; b\end{cases}\]</span></p><h4 id="标准化方法">标准化方法</h4><ul><li><p><strong>方法一</strong>（适用于无负数情况）： <spanclass="math display">\[z_{ij} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^{n} x_{ij}^2}}\]</span></p></li><li><p><strong>方法二</strong>（存在负数时）： <spanclass="math display">\[z_{ij} = \frac{x_{ij} - \min\{x_{1j}, x_{2j}, \ldots,x_{nj}\}}{\max\{x_{1j}, x_{2j}, \ldots, x_{nj}\} - \min\{x_{1j}, x_{2j},\ldots, x_{nj}\}}\]</span></p></li></ul><hr /><h3 id="计算概率矩阵">计算概率矩阵</h3><p>从标准化矩阵出发，计算各样本在每个指标下所占的比重，构成概率矩阵：</p><p><span class="math display">\[p_{ij} = \frac{z_{ij}}{\sum_{i=1}^{n} z_{ij}}\]</span> 这样确保每一列（每个指标）的概率和为1。</p><hr /><h3 id="计算信息熵与熵权">计算信息熵与熵权</h3><h4 id="计算信息熵">计算信息熵</h4><p>计算信息熵：</p><p><span class="math display">\[e_j = -\frac{1}{\ln n} \sum_{i=1}^{n} p_{ij}\ln(p_{ij}) \quad(j=1,2,\ldots,m)\]</span> <em>注意：当 <span class="math inline">\(p_{ij}=0\)</span>时，约定 <span class="math inline">\(\ln(0)=0\)</span> 。</em></p><p>这里除以 <span class="math inline">\(\ln n\)</span> 使得 <spanclass="math inline">\(e_j\)</span> 被归一化到 <spanclass="math inline">\([0,1]\)</span> 区间。</p><h4 id="计算信息效用值">计算信息效用值</h4><p>定义信息效用值 <span class="math inline">\(d_j\)</span> 为： <spanclass="math display">\[d_j = 1 - e_j\]</span> 信息效用值越大，表示该指标包含的信息越多。</p><h4 id="归一化得熵权">归一化得熵权</h4><p>最终，将各指标的信息效用值归一化，得到熵权 <spanclass="math inline">\(\omega_j\)</span>： <span class="math display">\[\omega_j = \frac{d_j}{\sum_{j=1}^{m} d_j} \quad (j=1,2,\ldots,m)\]</span></p><h4 id="得分">得分</h4><p><span class="math display">\[score_i =  \sum_{j=1}^{m} w_{j}z_{ij} \quad (i=1,2,\ldots,n)\]</span></p><h2 id="示例代码">示例代码</h2><p>（转自<ahref="https://blog.csdn.net/m0_46246301/article/details/106735607">这篇帖子</a>）</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs matlab"><span class="hljs-comment">% 代码实战一下吧，如下是清风老师的课程里的例子，自己再敲了一下代码并简化了一丢丢？</span><br>clc,clear;<br><br><span class="hljs-comment">% 定义函数</span><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[lnP]</span> = <span class="hljs-title">My_log</span><span class="hljs-params">(P)</span></span><br>    <span class="hljs-comment">% 当矩阵P中的元素为0时，返回0</span><br>    n = <span class="hljs-built_in">size</span>(P,<span class="hljs-number">1</span>);<br>    m = <span class="hljs-built_in">size</span>(P,<span class="hljs-number">2</span>);<br>    lnP = <span class="hljs-built_in">zeros</span>(n,m);<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">1</span>:n*m<br>        <span class="hljs-keyword">if</span> P(<span class="hljs-built_in">i</span>) == <span class="hljs-number">0</span><br>            lnP(<span class="hljs-built_in">i</span>) = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">else</span><br>            lnP(<span class="hljs-built_in">i</span>) = <span class="hljs-built_in">log</span>(P(<span class="hljs-built_in">i</span>));<br>        <span class="hljs-keyword">end</span><br>    <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 传入三个参数：</span><br><span class="hljs-comment">% i为第几列,type为指标类型(1：极小型， 2：中间型， 3：区间型),A_col为对应列向量</span><br><span class="hljs-comment">% 返回正向化后的列向量A_col</span><br><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">[A_col]</span> = <span class="hljs-title">Positivization</span><span class="hljs-params">(i,type,A_col)</span></span><br>    <br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span> == <span class="hljs-number">1</span>        <span class="hljs-comment">% 极小型</span><br>        <span class="hljs-built_in">disp</span>([<span class="hljs-string">&#x27;第&#x27;</span>,num2str(<span class="hljs-built_in">i</span>),<span class="hljs-string">&#x27;列是极小型&#x27;</span>] )<br>        A_col = min_to_max(A_col);<br>    <span class="hljs-keyword">elseif</span> <span class="hljs-built_in">type</span> == <span class="hljs-number">2</span>    <span class="hljs-comment">% 中间型</span><br>        <span class="hljs-built_in">disp</span>([<span class="hljs-string">&#x27;第&#x27;</span>,num2str(<span class="hljs-built_in">i</span>),<span class="hljs-string">&#x27;列是中间型&#x27;</span>] )<br>        value = input(<span class="hljs-string">&#x27;请输入最佳值： &#x27;</span>);<br>        A_col = mid_to_max(A_col, value);<br>    <span class="hljs-keyword">elseif</span> <span class="hljs-built_in">type</span> == <span class="hljs-number">3</span>    <span class="hljs-comment">% 区间型</span><br>        <span class="hljs-built_in">disp</span>([<span class="hljs-string">&#x27;第&#x27;</span>,num2str(<span class="hljs-built_in">i</span>),<span class="hljs-string">&#x27;列是区间型&#x27;</span>] )<br>        l = input(<span class="hljs-string">&#x27;请输入区间的下界： &#x27;</span>);<br>        r = input(<span class="hljs-string">&#x27;请输入区间的上界： &#x27;</span>); <br>        A_col = inter_to_max(A_col, l, r);<br>    <span class="hljs-keyword">else</span><br>        <span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;没有这种类型的指标,请检查type是否输入错误&#x27;</span>)<br>    <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br><br><span class="hljs-comment">% 1、导入数据</span><br>load data_river.mat<br><span class="hljs-comment">% 2、正向化</span><br>[n,m] = <span class="hljs-built_in">size</span>(A);<br><span class="hljs-built_in">disp</span>([<span class="hljs-string">&#x27;该数据一共有&#x27;</span>,num2str(n),<span class="hljs-string">&#x27;个评价对象,有&#x27;</span>,num2str(m),<span class="hljs-string">&#x27;个评价指标&#x27;</span>])<br>jud = input([<span class="hljs-string">&#x27;这&#x27;</span> num2str(m) <span class="hljs-string">&#x27;个指标是否需要进行正向化处理，需要请输入1 ，不需要输入0：&#x27;</span>]);<br><span class="hljs-keyword">if</span> jud == <span class="hljs-number">1</span><br>    col_list = input(<span class="hljs-string">&#x27;请输入需要正向化的指标所在列组成的列表，如第2、3列，需输入[2,3]: &#x27;</span>);<br>    type_list = input(<span class="hljs-string">&#x27;请输入这些列的指标类型组成的列表（1：极小型， 2：中间型， 3：区间型): &#x27;</span>);<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">i</span> = <span class="hljs-number">1</span>:<span class="hljs-built_in">size</span>(col_list,<span class="hljs-number">2</span>)  <span class="hljs-comment">% 列</span><br>        A(:,col_list(<span class="hljs-built_in">i</span>)) = Positivization(col_list(<span class="hljs-built_in">i</span>),type_list(<span class="hljs-built_in">i</span>),A(:,col_list(<span class="hljs-built_in">i</span>)));<br>    <span class="hljs-keyword">end</span><br><span class="hljs-keyword">end</span><br><span class="hljs-comment">% 3、标准化</span><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">isempty</span>(<span class="hljs-built_in">find</span>(A &lt; <span class="hljs-number">0</span>))<br>    <span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;不存在负数，标准化后的矩阵为:&#x27;</span>)<br>    <span class="hljs-comment">% A_stand = A./ repmat(sum(A.*A).^(1/2),n,1) % 按列求和</span><br>    A_stand = (A - <span class="hljs-built_in">repmat</span>(<span class="hljs-built_in">min</span>(A),n,<span class="hljs-number">1</span>))./(<span class="hljs-built_in">repmat</span>(<span class="hljs-built_in">max</span>(A)-<span class="hljs-built_in">min</span>(A),n,<span class="hljs-number">1</span>)) <br><span class="hljs-keyword">else</span><br>    <span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;存在负数，标准化后的矩阵为:&#x27;</span>)<br>    A_stand = (A - <span class="hljs-built_in">repmat</span>(<span class="hljs-built_in">min</span>(A),n,<span class="hljs-number">1</span>))./(<span class="hljs-built_in">repmat</span>(<span class="hljs-built_in">max</span>(A)-<span class="hljs-built_in">min</span>(A),n,<span class="hljs-number">1</span>)) <br><span class="hljs-keyword">end</span><br><span class="hljs-comment">% 4、计算概率矩阵:相当于每个标准化后的指标归一化</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;概率矩阵为:&#x27;</span>)<br>P = A_stand./<span class="hljs-built_in">repmat</span>(sum(A_stand),n,<span class="hljs-number">1</span>)<br><span class="hljs-comment">% 5、计算每个指标的信息熵</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;指标的信息熵:&#x27;</span>)<br>E = -sum(P.*My_log(P))/<span class="hljs-built_in">log</span>(n)<br><span class="hljs-comment">% 6、计算权重</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;指标对应的权重:&#x27;</span>)<br>W = (<span class="hljs-number">1</span>-E)./sum(E)<br><span class="hljs-comment">% 归一化</span><br>W = W./sum(W)<br><span class="hljs-comment">% 7、计算最终得分</span><br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;最终综合得分为:&#x27;</span>)<br>score = sum(W.*A_stand,<span class="hljs-number">2</span>)<br>score_stand = score ./ sum(score);<br>[score_stand_sort, index] = <span class="hljs-built_in">sort</span>(score_stand, <span class="hljs-string">&#x27;descend&#x27;</span>);<br><span class="hljs-built_in">disp</span>(<span class="hljs-string">&#x27;最终名次为:&#x27;</span>)<br><span class="hljs-built_in">disp</span>(index)<br></code></pre></td></tr></table></figure><hr /><h2 id="扩展">扩展</h2><ol type="1"><li><strong>修正TOPSIS法</strong>：可利用熵权法修正TOPSIS法，使决策更加客观。</li><li><strong>主客观权重综合</strong>：将客观赋权（熵权法）与主观赋权相结合，可获得更合理的权重分配。</li><li><strong>其他客观赋权方法</strong>：例如灰色关联分析法也可作为计算权重的方法。</li><li><strong>标准化方法选择</strong>：不同的标准化方法会得到不同的 (Z)矩阵，需根据实际情况选择合适方法。</li></ol><hr /><h2 id="模型总结">模型总结</h2><ul><li><strong>第一步</strong>：检查输入矩阵中是否存在负数，并进行必要的标准化处理，确保数据非负。</li><li><strong>第二步</strong>：计算每个指标下每个样本的比重，构成概率矩阵。</li><li><strong>第三步</strong>：利用概率矩阵计算各指标的信息熵，然后计算信息效用值，最后归一化得到各指标的权重。</li></ul><hr />]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
      <category>数学建模</category>
      
      <category>评价类模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost公式文档</title>
    <link href="/2025/03/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/2025-03-31-XGBoost&#39;s_latex/"/>
    <url>/2025/03/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/2025-03-31-XGBoost&#39;s_latex/</url>
    
    <content type="html"><![CDATA[<h1 id="xgboost技术文档">XGBoost技术文档</h1><h2 id="参数设置">参数设置</h2><p>见代码</p><h2 id="涉及到的相关数学公式">涉及到的相关数学公式：</h2><ul><li><strong>模型目标函数</strong>：</li></ul><p><span class="math display">\[\text{Obj}(\theta)^{(t)} = \sum_{i=1}^n L(y_i, \hat{y}_i) + \sum_{k=1}^K\Omega(f_k)\]</span></p><ul><li><strong>正则项</strong></li></ul><p><span class="math display">\[\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2\]</span></p><ul><li><strong>目标函数近似</strong></li></ul><p><span class="math display">\[\text{Obj}(\theta)^{(t)} \approx \sum_{i=1}^n \left[ g_i f_t(x_i) +\frac{1}{2} h_i f_t^2(x_i) \right] + \Omega(f_t)\]</span></p><ul><li><strong>增益</strong></li></ul><p><span class="math display">\[\text{Gain} = \frac{1}{2} \left[ \frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i\in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \inI_R} h_i + \lambda} - \frac{(\sum_{i \in I} g_i)^2}{\sum_{i \in I} h_i +\lambda} \right] - \gamma\]</span></p><ul><li><strong>对数损失</strong></li></ul><p><span class="math display">\[\text{LogLoss} = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log p_i + (1-y_i)\log(1-p_i) \right]\]</span></p><ul><li><strong>分类错误率</strong></li></ul><p><span class="math display">\[\text{Error} = \frac{1}{N} \sum_{i=1}^N \mathbb{I}(\hat{y}_i \neq y_i)\]</span></p>]]></content>
    
    
    <categories>
      
      <category>机器学习</category>
      
      <category>公式推导</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
